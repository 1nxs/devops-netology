# Домашнее задание к занятию "6.6. Troubleshooting"


## Задача 1

Перед выполнением задания ознакомьтесь с документацией по [администрированию MongoDB](https://docs.mongodb.com/manual/administration/).

Пользователь (разработчик) написал в канал поддержки, что у него уже 3 минуты происходит CRUD операция в MongoDB и её 
нужно прервать. 

Вы как инженер поддержки решили произвести данную операцию:
- напишите список операций, которые вы будете производить для остановки запроса пользователя
- предложите вариант решения проблемы с долгими (зависающими) запросами в MongoDB

<details><summary>вместо intro</summary>
<br>
Уже традиционно:

- сделан тестовый стенд под vagrant+vbox
- в [docker-compose](./vm/ansible/stack/docker-compose.yaml) живёт MongoDB и Redis
- лень менять докер файл, для MySQL и PostgreSQL можно взять из старых лаб (взял)

</details>

### Ответ
**Ч.1** Описать что делать с текущей проблемой:

```shell
[vagrant@server66 stack]$ sudo docker exec -it mongodb bash
root@b57a43177faf:/# mongosh
# Идём смотреть что вообще выполняется, смотрим синтаксис :)
db.currentOp()
# Теперь уже более осмысленно, среди active и временем более 180
db.currentOp(
  { 
    "active" : true,
    "secs_running" : { "$gt" : 180 }
    }
)
# Получив opid из предыдущей операции используем db.killOp(opid)
db.killOp(<opid)
``` 
**Ч.2** Предложить решение \ предупредить повторные итерации:

Не уверен про последовательность, тк с БД не каждый день работаю, но логика такая:
- Если не забита очередь IOps диска, то при помощи `explain ()` попытаться изучить запрос перед тем как его килять
- Если всёж забита, то ограничить время выполнения запроса `maxTimeMS()`
- Включить отслеживание медленных запросов `db.setProfilingLevel(1, { slowms: xx })` (выставить чуть ниже п.2, чтоб отслеживать перед тем как дропнет)
- Построить индекс, еще раз с помощью `explain()` оценить "как похорошела база"
- ~~Отстрелить разработчика положившего базу (в приоритете)~~

## Задача 2

Перед выполнением задания познакомьтесь с документацией по [Redis latency troobleshooting](https://redis.io/topics/latency).

Вы запустили инстанс Redis для использования совместно с сервисом, который использует механизм TTL. 
Причем отношение количества записанных key-value значений к количеству истёкших значений есть величина постоянная и
увеличивается пропорционально количеству реплик сервиса. 

При масштабировании сервиса до N реплик вы увидели, что:
- сначала рост отношения записанных значений к истекшим
- Redis блокирует операции записи

Как вы думаете, в чем может быть проблема?
### Ответ
Вот сначала думал что требуется оперативки подкинуть, **НО** если кратко то:\
Очистка от старых ключей происходит каждые 100 мс, по-умолчанию за раз вычищается не более 20 ключей. \
Если в бд более 25% истекающих ключей в данную секунду, то Redis блокируется, пока количество истекших ключей не станет менее 25%

Стало быть параметр `ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP` необходимо увеличить, как и оперативку.

<details><summary>копипаст из мануала по теме</summary>

**Latency generated by expires**

Redis evict expired keys in two ways:

- One lazy way expires a key when it is requested by a command, but it is found to be already expired.\
- One active way expires a few keys every 100 milliseconds.\

The active expiring is designed to be adaptive. An expire cycle is started every 100 milliseconds (10 times per second), and will do the following:

- Sample `ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP` keys, evicting all the keys already expired.\
- If the more than 25% of the keys were found expired, repeat.\

Given that `ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP` is set to 20 by default, and the process is performed ten times per second, usually just 200 keys per second are actively expired. This is enough to clean the DB fast enough even when already expired keys are not accessed for a long time, so that the lazy algorithm does not help. At the same time expiring just 200 keys per second has no effects in the latency a Redis instance.

However the algorithm is adaptive and will loop if it finds more than 25% of keys already expired in the set of sampled keys. But given that we run the algorithm ten times per second, this means that the unlucky event of more than 25% of the keys in our random sample are expiring at least in the same second.

Basically this means that if the database has many, many keys expiring in the same second, and these make up at least 25% of the current population of keys with an expire set, Redis can block in order to get the percentage of keys already expired below 25%.

This approach is needed in order to avoid using too much memory for keys that are already expired, and usually is absolutely harmless since it's strange that a big number of keys are going to expire in the same exact second, but it is not impossible that the user used EXPIREAT extensively with the same Unix time.

In short: be aware that many keys expiring at the same moment can be a source of latency.
</details>

Так же стоит, наверное, еще поглядеть в сторону документации по [репликации](https://redis.io/docs/management/replication/):

текущий мастер перестанет принимать запросы, если не наберется достаточное кол-во реплик со значениями не превышающими заданный `lag` (максимальное кол-во секунд с последнего опроса реплики)
~~(полечил сейчас свой террасофт BPM на этот счёт)~~

живёт оно в redis.conf:
- min-replicas-to-write <number of replicas>
- min-replicas-max-lag <number of seconds>

 
## Задача 3

Вы подняли базу данных MySQL для использования в гис-системе. При росте количества записей, в таблицах базы,
пользователи начали жаловаться на ошибки вида:
```python
InterfaceError: (InterfaceError) 2013: Lost connection to MySQL server during query u'SELECT..... '
```

Как вы думаете, почему это начало происходить и как локализовать проблему?

Какие пути решения данной проблемы вы можете предложить?
### Ответ
Как вы думаете, почему это начало происходить и как локализовать проблему?

1. Большой запрос, который не успевает передаться\
В связи с увеличением БД, выросло время ответа пользователю(выдается больше записей) \
Сервер должен выдать ответ в определенный период, иначе возникает ошибка "during query..."
Изменить это время можно покрутив 
[net_read_timeout=30(default)](https://dev.mysql.com/doc/refman/5.7/en/error-lost-connection.html)

2. Большой запрос, тип BLOB\
Если в ошибке есть `ER_NET_PACKET_TOO_LARGE`, то вероятно проблема в превышении размера сообщения. \
Необходимо увеличить параметр `max_allowed_packet`

3. Медленное соединение \
~~виноват сетевик~~ `SHOW GLOBAL STATUS LIKE 'Aborted_connects'` проверить после нескольких обрывов, если увеличивается, то изменить значение `connect_timeout`


Какие пути решения данной проблемы вы можете предложить?
- Создать индекс для запрашиваемых данных
- Использовать вертикальное и горизонтальное шардирование
- Увеличить таймаут в настройке `net_read_timeout`
- расширить максимальное кол-во соединений [max_connections](https://www.opennet.ru/docs/RUS/sql_error/chap10.html)

## Задача 4


Вы решили перевести гис-систему из задачи 3 на PostgreSQL, так как прочитали в документации, что эта СУБД работает с 
большим объемом данных лучше, чем MySQL.

После запуска пользователи начали жаловаться, что СУБД время от времени становится недоступной. В dmesg вы видите, что:

`postmaster invoked oom-killer`

Как вы думаете, что происходит?

Как бы вы решили данную проблему?

### Ответ
Что есть `oom-killer` - это процесс, который отвечает за завершение приложения, чтобы спасти ядро от сбоя.\
Происходит конец выделенной серверу оперативки, и дабы не упасть окончательно - OOM прибивает PGSQL\
Ничего хорошего в прибивании процесса `postgres` нет, дабы чревато recovery.\
**upd:** Пока писал, вспомнил что еще место на swap разделе тоже может послужить причиной.
Что сделать:
- корректность настройки vm имеет смысл проверить сразу
- далее ограничить жор ресурсов `pgsql`
  - Shared_buffers (integer)
  - Work_mem (integer)
  - Maintenance_work_mem (integer)
  - Effective_cache_size (integer)
- проверить\произвести настройку swap раздела и oom-killer
своп расширить, oom-killer можно тюнинговать
- ~~найти того кто написал запрос который регулярно кладёт базу и отстрелить ))~~
